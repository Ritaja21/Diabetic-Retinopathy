{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:39:44.283083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 17:39:44.363008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733486984.425111   47518 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733486984.443691   47518 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 17:39:44.528206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List physical devices\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Optionally, list details\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecord:\n",
    "\n",
    "    @staticmethod\n",
    "    def image_to_tfrecord(image):\n",
    "        # Read the image\n",
    "        if not tf.is_tensor(image):\n",
    "            image = tf.convert_to_tensor(image, dtype=tf.uint8)\n",
    "\n",
    "        # Ensure the image has 3 channels (e.g., RGB)\n",
    "        # Ensure the image has 3 channels (e.g., RGB)\n",
    "        if len(image.shape) == 2:  # Grayscale image\n",
    "            image = tf.expand_dims(image, axis=-1)  # Add channel dimension (height, width -> height, width, 1)\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        elif image.shape[-1] != 3:\n",
    "            raise ValueError(\"Input image must have 3 channels (RGB).\")\n",
    "\n",
    "        # Encode the image as JPEG bytes\n",
    "        image_raw = tf.io.encode_jpeg(image).numpy()\n",
    "\n",
    "        # Serialize into TFRecord format\n",
    "        feature = {\n",
    "            'image_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_raw])),\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        return example.SerializeToString()\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        feature_description = {\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        parsed = tf.io.parse_single_example(tfrecord, feature_description)\n",
    "        image = tf.io.decode_image(parsed['image_raw'], channels=3)\n",
    "        image = tf.image.resize(image, (224, 224))  # Adjust size to model input\n",
    "        image = image / 255.0  # Normalize if needed\n",
    "        return tf.expand_dims(image, axis=0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FundusPreprocessor:\n",
    "    def __init__(self, target_diameter=900, threshold_ratio=0.05, circular_diameter=6, filter_size=3,\n",
    "                 normalization_filter_size=20):\n",
    "        # Initialize the two component classes\n",
    "        self.roi_extractor = FundusROIextractor(target_diameter, threshold_ratio, circular_diameter)\n",
    "        self.illumination_equalizer = IlluminationEqualizer(filter_size, normalization_filter_size)\n",
    "        self.hist_eq = HistogramEqualizer()\n",
    "        self.ad_hist = AdaptiveHistogram()\n",
    "\n",
    "    def process(self, image):\n",
    "        roi_green = self.roi_extractor.process(image)\n",
    "\n",
    "        normalized_green = self.illumination_equalizer.normalize_image(roi_green)\n",
    "\n",
    "        green_hist = self.hist_eq.equalize_histogram(normalized_green)\n",
    "\n",
    "        ad_green = self.ad_hist.apply_clahe(green_hist)\n",
    "\n",
    "        return ad_green\n",
    "\n",
    "\n",
    "class FundusROIextractor:\n",
    "    def __init__(self, target_diameter=900, threshold_ratio=0.05, circular_diameter=6):\n",
    "        self.target_diameter = target_diameter\n",
    "        self.threshold_ratio = threshold_ratio\n",
    "        self.circular_diameter = circular_diameter\n",
    "\n",
    "    # green channel extraction\n",
    "    def extract_green_channel(self, image):\n",
    "        return image[:, :, 1]\n",
    "\n",
    "    def apply_global_threshold(self, image):\n",
    "        max_intensity = np.max(image)\n",
    "        threshold_value = int(self.threshold_ratio * max_intensity)\n",
    "        _, threshold_image = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "        return threshold_image\n",
    "\n",
    "    def morphological_operations(self, image):\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "        # Opening (Erosion followed by Dilation)\n",
    "        opened_image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Closing (Dilation followed by Erosion)\n",
    "        closed_image = cv2.morphologyEx(opened_image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        return closed_image\n",
    "\n",
    "    def resize_roi(self, image):\n",
    "        # resizing using cubic interpolation\n",
    "        h, w = image.shape\n",
    "        current_diameter = min(h, w)\n",
    "        scale_factor = self.target_diameter / current_diameter\n",
    "        new_size = (int(w * scale_factor), int(h * scale_factor))\n",
    "        resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "    def circular_corrosion(self, image):\n",
    "        struct_element = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (self.circular_diameter, self.circular_diameter))\n",
    "        eroded_image = cv2.erode(image, struct_element)\n",
    "\n",
    "        return eroded_image\n",
    "\n",
    "    def apply_mask(self, image, mask):\n",
    "        mask = mask.astype(np.uint8)\n",
    "\n",
    "        # Resize mask to match the original image size if necessary\n",
    "        if mask.shape != image.shape:\n",
    "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Apply bitwise_and to mask the original image\n",
    "        masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "        return masked_image\n",
    "\n",
    "    def process(self, image):\n",
    "        # Step 2: Extract the green channel\n",
    "        green_channel = self.extract_green_channel(image)\n",
    "\n",
    "        # Step 3: Apply global thresholding\n",
    "        thresholded_image_g = self.apply_global_threshold(green_channel)\n",
    "\n",
    "        # Step 4: Perform morphological operations (opening and closing)\n",
    "        processed_image_g = self.morphological_operations(thresholded_image_g)\n",
    "\n",
    "        # Step 5: Resize the ROI to a uniform size (e.g., 900 pixels diameter)\n",
    "        resized_roi_g = self.resize_roi(processed_image_g)\n",
    "\n",
    "        # Step 6: Apply circular corrosion to remove edge noise\n",
    "        final_roi_green = self.circular_corrosion(resized_roi_g)\n",
    "\n",
    "        masked_green = self.apply_mask(green_channel, final_roi_green)\n",
    "\n",
    "        return masked_green\n",
    "\n",
    "\n",
    "class IlluminationEqualizer:\n",
    "    def __init__(self, filter_size=3, normalization_filter_size=20):\n",
    "        self.filter_size = filter_size\n",
    "        self.normalization_filter_size = normalization_filter_size\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        min_val = np.min(image)\n",
    "        clipped_image = image - min_val\n",
    "        max_val = np.max(clipped_image)\n",
    "        normalized_image = cv2.normalize(clipped_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        return normalized_image\n",
    "\n",
    "\n",
    "class HistogramEqualizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def equalize_histogram(self, image):\n",
    "        return cv2.equalizeHist(image)\n",
    "\n",
    "\n",
    "class AdaptiveHistogram:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "    def apply_clahe(self, image):\n",
    "        return self.clahe.apply(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/media/mydisk/ICDCIT/Splitdataset/2/54_left.jpeg\"\n",
    "image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {0: \"No DR\", 1: \"Mild\", 2: \"Moderate\", 3: \"Severe\", 4: \"Proliferative DR\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_tfrecord = TFRecord.image_to_tfrecord(image)\n",
    "input_image = TFRecord.parse_tfrecord(serialized_tfrecord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predictions without preprocessing:\n",
      "Raw probabilities: [[0.3894   0.005157 0.508    0.01758  0.0798  ]]\n",
      "Predicted classes: [2]\n",
      "Predicted labels: ['Moderate']\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"/media/mydisk/ICDCIT/Diabetic Retinopathy/Diabetic-Retinopathy/best_model.keras\")\n",
    "prediction = model.predict(input_image)\n",
    "predicted_class = np.argmax(prediction, axis=1)  # Multi-class\n",
    "predicted_labels = [class_labels[i] for i in predicted_class]\n",
    "\n",
    "print(\"Predictions without preprocessing:\")\n",
    "print(\"Raw probabilities:\", prediction)\n",
    "print(\"Predicted classes:\", predicted_class)\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = FundusPreprocessor()\n",
    "preprocessed = preprocessor.process(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_tfrecord_preprocessed = TFRecord.image_to_tfrecord(preprocessed)\n",
    "input_image_preprocessed = TFRecord.parse_tfrecord(serialized_tfrecord_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predictions with preprocessing:\n",
      "Raw probabilities: [[0.0274  0.10266 0.754   0.02454 0.0911 ]]\n",
      "Predicted classes: [2]\n",
      "Predicted labels: ['Moderate']\n"
     ]
    }
   ],
   "source": [
    "model_preprocessed = load_model(\"/media/mydisk/ICDCIT/Diabetic Retinopathy/Diabetic-Retinopathy/best_model.keras\")\n",
    "prediction_preprocessed = model_preprocessed.predict(input_image_preprocessed)\n",
    "\n",
    "predicted_class_preprocessed = np.argmax(prediction_preprocessed, axis=1)  # Multi-class\n",
    "predicted_labels_preprocessed = [class_labels[i] for i in predicted_class_preprocessed]\n",
    "\n",
    "print(\"Predictions with preprocessing:\")\n",
    "print(\"Raw probabilities:\", prediction_preprocessed)\n",
    "print(\"Predicted classes:\", predicted_class_preprocessed)\n",
    "print(\"Predicted labels:\", predicted_labels_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
